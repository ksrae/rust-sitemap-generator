use std::collections::{HashSet, VecDeque};
use std::fs::File;
use std::io::BufWriter;

use dialoguer::{theme::ColorfulTheme, Input, Select};
use quick_xml::events::{BytesDecl, BytesEnd, BytesStart, BytesText, Event};
use quick_xml::Writer;
use scraper::{Html, Selector};
use url::{ParseError, Url};
use futures::future::join_all;

// --- ì„¤ì • êµ¬ì¡°ì²´ (String ì†Œìœ  ë°©ì‹ìœ¼ë¡œ ë³€ê²½) ---
struct SitemapOptions {
    base_url: String,
    output_file: String,
    changefreq: String,
    lastmod_option: LastmodOption,
    priority: Option<f32>,
    session_params_to_remove: Vec<String>,
}

enum LastmodOption {
    None,
    Exact(String),
}
// --- ì„¤ì • êµ¬ì¡°ì²´ ë ---

// clean_url, crawl_site, generate_xml í•¨ìˆ˜ëŠ” ì´ì „ ì½”ë“œì™€ ë™ì¼í•©ë‹ˆë‹¤.
// (ì•„ë˜ì— ì „ì²´ ì½”ë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.)

fn clean_url(url: &Url, params_to_remove: &[String]) -> Result<Url, ParseError> {
    let mut cleaned_url = url.clone();
    cleaned_url.set_fragment(None);

    let params_to_remove_str: Vec<&str> = params_to_remove.iter().map(AsRef::as_ref).collect();

    let new_pairs: Vec<(String, String)> = cleaned_url
        .query_pairs()
        .filter(|(key, _)| !params_to_remove_str.contains(&key.as_ref()))
        .map(|(k, v)| (k.into_owned(), v.into_owned()))
        .collect();

    if new_pairs.is_empty() {
        cleaned_url.set_query(None);
    } else {
        cleaned_url.query_pairs_mut().clear().extend_pairs(new_pairs);
    }

    Ok(cleaned_url)
}

async fn crawl_site(
    start_url: &Url,
    session_params: &[String],
) -> Result<HashSet<String>, Box<dyn std::error::Error>> {
    println!("'{}' ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...", start_url);

    let base_domain = start_url.domain().ok_or("ì‹œì‘ URLì— ë„ë©”ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")?.to_string();
    let mut urls_to_visit = VecDeque::new();
    urls_to_visit.push_back(start_url.clone());

    let mut visited = HashSet::new();
    let mut in_flight = HashSet::new();
    let client = reqwest::Client::new();
    let concurrency: usize = 20;

    while !urls_to_visit.is_empty() {
        // Prepare a batch of up to `concurrency` futures
        let mut batch = Vec::new();

        while batch.len() < concurrency {
            if let Some(current_url) = urls_to_visit.pop_front() {
                match clean_url(&current_url, session_params) {
                    Ok(cleaned_url) => {
                        // domain check
                        if let Some(domain) = cleaned_url.domain() {
                            if domain != base_domain { continue; }
                        } else { continue; }

                        let cleaned_str = cleaned_url.to_string();
                        if visited.contains(&cleaned_str) || in_flight.contains(&cleaned_str) { continue; }

                        in_flight.insert(cleaned_str.clone());
                        let client = client.clone();

                        batch.push(tokio::spawn(async move {
                            let mut found_urls: Vec<Url> = Vec::new();

                            let res = match client.get(cleaned_url.clone()).send().await {
                                Ok(response) => {
                                    if !response.status().is_success() {
                                        Err(format!("Status not success: {}", response.status()))
                                    } else {
                                        let content_type = response.headers().get("content-type").and_then(|v| v.to_str().ok()).unwrap_or("");
                                        if !content_type.contains("text/html") {
                                            Err("Not an HTML page".to_string())
                                        } else {
                                            match response.text().await {
                                                Ok(body) => {
                                                    println!("  [íƒìƒ‰ ì™„ë£Œ] {}", cleaned_url);
                                                    let document = Html::parse_document(&body);
                                                    let link_selector = Selector::parse("a[href]").unwrap();
                                                    for element in document.select(&link_selector) {
                                                        if let Some(href) = element.value().attr("href") {
                                                            if let Ok(mut absolute_url) = cleaned_url.join(href) {
                                                                absolute_url.set_fragment(None);
                                                                found_urls.push(absolute_url);
                                                            }
                                                        }
                                                    }
                                                    Ok(found_urls)
                                                }
                                                Err(e) => Err(format!("Body read error: {}", e)),
                                            }
                                        }
                                    }
                                }
                                Err(e) => Err(format!("Request error: {}", e)),
                            };

                            (cleaned_str, res)
                        }));
                    }
                    Err(_) => {
                        // skip invalid URL
                    }
                }
            } else {
                break;
            }
        }

        if batch.is_empty() { break; }

        let results = join_all(batch).await;

        for handle in results {
            if let Ok((cleaned_str, result)) = handle {
                in_flight.remove(&cleaned_str);
                match result {
                    Ok(found_urls) => {
                        visited.insert(cleaned_str.clone());
                        for url in found_urls {
                            if let Some(domain) = url.domain() {
                                if domain == base_domain {
                                    let url_str = url.to_string();
                                    if !visited.contains(&url_str) && !in_flight.contains(&url_str) {
                                        urls_to_visit.push_back(url);
                                    }
                                }
                            }
                        }
                    }
                    Err(e) => {
                        println!("  [ì—ëŸ¬] {} í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}", cleaned_str, e);
                    }
                }
            }
        }
    }

    Ok(visited)
}

fn generate_xml(
    options: &SitemapOptions,
    urls: &HashSet<String>,
) -> Result<(), Box<dyn std::error::Error>> {
    let file = File::create(&options.output_file)?;
    let mut writer = Writer::new(BufWriter::new(file));

    writer.write_event(Event::Decl(BytesDecl::new("1.0", Some("UTF-8"), None)))?;

    let mut urlset_tag = BytesStart::new("urlset");
    urlset_tag.push_attribute(("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9"));
    writer.write_event(Event::Start(urlset_tag))?;

    for url in urls {
        writer.write_event(Event::Start(BytesStart::new("url")))?;
        writer.write_event(Event::Start(BytesStart::new("loc")))?;
        writer.write_event(Event::Text(BytesText::new(url)))?;
        writer.write_event(Event::End(BytesEnd::new("loc")))?;

        if let LastmodOption::Exact(date) = &options.lastmod_option {
            writer.write_event(Event::Start(BytesStart::new("lastmod")))?;
            writer.write_event(Event::Text(BytesText::new(date)))?;
            writer.write_event(Event::End(BytesEnd::new("lastmod")))?;
        }

        writer.write_event(Event::Start(BytesStart::new("changefreq")))?;
        writer.write_event(Event::Text(BytesText::new(&options.changefreq)))?;
        writer.write_event(Event::End(BytesEnd::new("changefreq")))?;

        if let Some(p) = options.priority {
            writer.write_event(Event::Start(BytesStart::new("priority")))?;
            writer.write_event(Event::Text(BytesText::new(&p.to_string())))?;
            writer.write_event(Event::End(BytesEnd::new("priority")))?;
        }
        writer.write_event(Event::End(BytesEnd::new("url")))?;
    }

    writer.write_event(Event::End(BytesEnd::new("urlset")))?;
    println!("\n'{}' ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", options.output_file);
    Ok(())
}


// --- ëŒ€í™”í˜•ìœ¼ë¡œ ë³€ê²½ëœ main í•¨ìˆ˜ ---
#[tokio::main]
async fn main() {
    let theme = ColorfulTheme::default();
    println!("ğŸš€ XML Sitemap ìƒì„±ê¸° (ëŒ€í™”í˜• ëª¨ë“œ)");
    println!("------------------------------------");

    // 1. URL ì…ë ¥ë°›ê¸°
    let base_url: String = Input::with_theme(&theme)
        .with_prompt("1. ì‚¬ì´íŠ¸ë§µì„ ìƒì„±í•  URLì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: https://example.com)")
        .validate_with(|input: &String| -> Result<(), &str> {
            if Url::parse(input).is_ok() {
                Ok(())
            } else {
                Err("ìœ íš¨í•œ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. 'http://' ë˜ëŠ” 'https://'ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.")
            }
        })
        .interact_text()
        .unwrap();

    // 2. Page changing frequency ì„ íƒ
    let changefreqs = &["daily", "weekly", "monthly", "yearly", "never"];
    let changefreq_selection = Select::with_theme(&theme)
        .with_prompt("2. í˜ì´ì§€ ë³€ê²½ ë¹ˆë„ë¥¼ ì„ íƒí•˜ì„¸ìš”")
        .items(changefreqs)
        .default(1) // 'weekly'ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ
        .interact()
        .unwrap();
    let changefreq = changefreqs[changefreq_selection].to_string();

    // 3. Last modified date ì„ íƒ
    let lastmod_options = &["ì§€ì • ì•ˆí•¨ (Don't specify)", "ì •í™•í•œ ë‚ ì§œ ì‚¬ìš© (Use exact value)"];
    let lastmod_selection = Select::with_theme(&theme)
        .with_prompt("3. ë§ˆì§€ë§‰ ìˆ˜ì •ì¼ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”")
        .items(lastmod_options)
        .default(0)
        .interact()
        .unwrap();
    
    let lastmod_option = if lastmod_selection == 1 {
        let date_str: String = Input::with_theme(&theme)
            .with_prompt("  - ì‚¬ìš©í•  ë‚ ì§œë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYY-MM-DD í˜•ì‹)")
            .interact_text()
            .unwrap();
        LastmodOption::Exact(date_str)
    } else {
        LastmodOption::None
    };

    // 4. Page priority ì„ íƒ
    let priority_options = &["ì§€ì • ì•ˆí•¨ (Don't specify)", "ì •í™•í•œ ê°’ ì‚¬ìš© (Use exact value)"];
    let priority_selection = Select::with_theme(&theme)
        .with_prompt("4. í˜ì´ì§€ ìš°ì„ ìˆœìœ„ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”")
        .items(priority_options)
        .default(1)
        .interact()
        .unwrap();

    let priority: Option<f32> = if priority_selection == 1 {
        Some(Input::with_theme(&theme)
            .with_prompt("  - ìš°ì„ ìˆœìœ„ ê°’ì„ ì…ë ¥í•˜ì„¸ìš” (0.0 ~ 1.0)")
            .default(0.5)
            .validate_with(|input: &f32| -> Result<(), &str> {
                if (0.0..=1.0).contains(input) {
                    Ok(())
                } else {
                    Err("ê°’ì€ 0.0ê³¼ 1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
                }
            })
            .interact()
            .unwrap())
    } else {
        None
    };

    // 5. ì„¸ì…˜ íŒŒë¼ë¯¸í„° ì…ë ¥
    let session_params_str: String = Input::with_theme(&theme)
        .with_prompt("5. URLì—ì„œ ì œê±°í•  ì„¸ì…˜ íŒŒë¼ë¯¸í„°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)")
        .default("sid,phpsessid".to_string())
        .interact_text()
        .unwrap();
    let session_params_to_remove: Vec<String> = session_params_str
        .split(',')
        .map(|s| s.trim().to_string())
        .filter(|s| !s.is_empty())
        .collect();

    // ëª¨ë“  ì˜µì…˜ì„ êµ¬ì¡°ì²´ì— ë‹´ê¸°
    let options = SitemapOptions {
        base_url: base_url.clone(),
        output_file: "sitemap.xml".to_string(),
        changefreq,
        lastmod_option,
        priority,
        session_params_to_remove,
    };

    println!("\n------------------------------------");
    println!("ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.");

    // í¬ë¡¤ë§ ë° íŒŒì¼ ìƒì„± ì‹¤í–‰
    let start_url = Url::parse(&base_url).unwrap();
    match crawl_site(&start_url, &options.session_params_to_remove).await {
        Ok(found_urls) => {
            println!("\nì´ {}ê°œì˜ í˜ì´ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. '{}' íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...", found_urls.len(), options.output_file);
            if let Err(e) = generate_xml(&options, &found_urls) {
                eprintln!("XML íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {}", e);
            }
        }
        Err(e) => {
            eprintln!("í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {}", e);
        }
    }
}